{"cells":[{"cell_type":"markdown","metadata":{"id":"Azr6h6_ltBB6"},"source":["# **Import Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9123,"status":"ok","timestamp":1700312263936,"user":{"displayName":"Mohamed Orfy","userId":"12556977189253546046"},"user_tz":-120},"id":"OX0BYC7qtK8M","outputId":"bd3ca09b-7b8a-4a76-881c-84e93c74d666"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: idx2numpy in /usr/local/lib/python3.10/dist-packages (1.2.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from idx2numpy) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from idx2numpy) (1.16.0)\n"]}],"source":["!pip install idx2numpy\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import idx2numpy\n"]},{"cell_type":"markdown","metadata":{"id":"voNAUMd5ts3Q"},"source":["# **Data Preparation**"]},{"cell_type":"markdown","metadata":{"id":"j4jD5OK0y2_Z"},"source":["\n","\n","*  MNIST data is loaded and split into training and validation sets.\n","* DataLoader objects are created for both sets to facilitate batch processing\n","during training and validation.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1eBrHnnufv_"},"outputs":[],"source":["# Load MNIST data using idx2numpy\n","testLabel = idx2numpy.convert_from_file('/content/t10k-labels.idx1-ubyte')\n","train = idx2numpy.convert_from_file('/content/train-images.idx3-ubyte')\n","testData = idx2numpy.convert_from_file('/content/t10k-images.idx3-ubyte')\n","trainLabel = idx2numpy.convert_from_file('/content/train-labels.idx1-ubyte')\n","\n","# Data preprocessing: normalize and convert to PyTorch tensors\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","data = torch.from_numpy(train).data.view(-1, 28 * 28).float()\n","labels = torch.from_numpy(trainLabel)\n","# Split the data into training and validation sets\n","train_data, val_data, train_labels, val_labels = train_test_split(data, labels, stratify=labels, test_size=0.2, random_state=42)\n","train_dataset = TensorDataset(train_data, train_labels)\n","val_dataset = TensorDataset(val_data, val_labels)\n","# Create DataLoader objects for training and validation sets\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"Xvv4RG6IyE2x"},"source":["# **Neural Network Architecture**"]},{"cell_type":"markdown","metadata":{"id":"CPeTpL9LzaYe"},"source":["* A simple neural network with three fully connected layers (fc1, fc2, fc3).\n","* ReLU activation functions are used between hidden layers.\n","* The model is designed for image classification with 10 output classes.\n","* Log-softmax is applied to the output for training with negative log likelihood loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzXwC_DbyIGJ"},"outputs":[],"source":["# Define a simple neural network architecture\n","class CustomNet(nn.Module):\n","    def __init__(self):\n","        super(CustomNet, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.fc4 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        return x\n","# Instantiate the model\n","model = CustomNet()\n"]},{"cell_type":"markdown","metadata":{"id":"2Ia91Cfu7D9g"},"source":["# **Training Function**"]},{"cell_type":"markdown","metadata":{"id":"-h06D-Mvz1L2"},"source":["* The training process involves iterating over batches, performing forward and backward passes, and updating the model parameters.\n","* Training accuracy and losses are tracked."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FSIIfXH7WkD"},"outputs":[],"source":["# Training Process\n","\n","# Define training hyperparameters and loss function\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","criterion = nn.CrossEntropyLoss()\n","\n","def train(epoch):\n","    model.train()\n","    correct_train = 0\n","    total_train = 0\n","    running_loss = 0.0\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = output.max(1)\n","        total_train += target.size(0)\n","        correct_train += predicted.eq(target).sum().item()\n","\n","    train_accuracy = 100. * correct_train / total_train\n","    train_losses.append(running_loss / len(train_loader))\n","    train_accuracies.append(train_accuracy)\n","\n","    #print('Train Epoch: {} \\tLoss: {:.6f} \\tAccuracy: {:.2f}%'.format(epoch, train_losses[-1], train_accuracy))\n"]},{"cell_type":"markdown","metadata":{"id":"bO8ioY0jLkdv"},"source":["\n","\n","# **Validation function**"]},{"cell_type":"markdown","metadata":{"id":"OkYn9Kaj0F3i"},"source":["* The model is evaluated on the validation set without updating parameters.\n","* Validation accuracy and losses are tracked."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkVZithVMBtM"},"outputs":[],"source":["def validate():\n","    model.eval()\n","    correct_val = 0\n","    total_val = 0\n","    val_loss = 0.0\n","\n","    with torch.no_grad():\n","        for data, target in val_loader:\n","            output = model(data)\n","            val_loss += criterion(output, target).item()\n","            _, predicted = output.max(1)\n","            total_val += target.size(0)\n","            correct_val += predicted.eq(target).sum().item()\n","\n","    val_accuracy = 100. * correct_val / total_val\n","    val_losses.append(val_loss / len(val_loader))\n","    val_accuracies.append(val_accuracy)\n","\n","    print('Validation set: Average loss: {:.4f}, Accuracy: {:.2f}%'.format(val_losses[-1], val_accuracy))"]},{"cell_type":"markdown","metadata":{"id":"rg7-O2OWMVR5"},"source":["# **Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkrABIREMclU"},"outputs":[],"source":["train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","for epoch in range(1, 11):\n","    train(epoch)\n","validate()"]},{"cell_type":"markdown","metadata":{"id":"PZbP_ekGPqbI"},"source":["# **Plotting**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvW80mt0RhUl"},"outputs":[],"source":["def plot_loss(train_losses, val_losses,title):\n","    plt.plot(train_losses, label='Training loss')\n","    plt.plot(val_losses, label='Validation loss')\n","    plt.legend()\n","    plt.title(title)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.show()\n","\n","def plot_accuracy(train_accuracies, val_accuracies,title):\n","    plt.plot(train_accuracies, label='Training accuracy')\n","    plt.plot(val_accuracies, label='Validation accuracy')\n","    plt.legend()\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy (%)')\n","    plt.title(title)\n","    plt.show()\n","plt.figure(figsize=(12, 5))\n","plot_loss(train_losses, val_losses, \"Loss Plot\")\n","plt.figure(figsize=(12, 5))\n","plot_accuracy(train_accuracies, val_accuracies, \"Accuracy Plot\")"]},{"cell_type":"markdown","metadata":{"id":"dvLr1W2xmhib"},"source":["# **Analysis**"]},{"cell_type":"markdown","metadata":{"id":"aRDjD6oT0irC"},"source":["* Learning rates and batch sizes are varied to find the best hyperparameters.\n","* Models are trained for each combination, and the one with the best validation accuracy is selected."]},{"cell_type":"markdown","metadata":{"id":"fcHyAMdvozZh"},"source":["# 1-Changing Learning Rate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-L7kIWRbd7fe"},"outputs":[],"source":["learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n","\n","best_lr = None\n","best_val_accuracy = 0.0\n","\n","for lr in learning_rates:\n","    model = CustomNet()\n","    optimizer = optim.SGD(model.parameters(), lr=lr)\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    for epoch in range(1, 11):\n","        train(epoch)\n","        validate()\n","\n","    # Check if the current learning rate gives better validation accuracy\n","    if val_accuracies[-1] > best_val_accuracy:\n","        best_lr = lr\n","        best_val_accuracy = val_accuracies[-1]\n","\n","# Now, train the model with the best learning rate\n","model = CustomNet()\n","optimizer = optim.SGD(model.parameters(), lr=best_lr)\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","for epoch in range(1, 11):\n","    train(epoch)\n","    validate()\n"]},{"cell_type":"markdown","metadata":{"id":"mH8605kuo8-P"},"source":["# Plotting for the best learning rate\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPv8QkeVoipR"},"outputs":[],"source":["plt.figure(figsize=(12, 5))\n","plot_loss(train_losses, val_losses,f\"Losses for Learning Rate: {best_lr}\")\n","plt.figure(figsize=(12, 5))\n","plot_accuracy(train_accuracies, val_accuracies, f\"Accuracies for Learning Rate: {best_lr}\")\n","print(f\"Best Learning Rate: {best_lr}, Final Validation Accuracy: {best_val_accuracy:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"7WZBsxSapNHX"},"source":["# 2-Changing Batch Size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kf31TWjMoM3l"},"outputs":[],"source":["#Changing the batch size and plot best one\n","batch_sizes = [32, 64, 128, 256, 512]\n","\n","best_batch_size = None\n","best_val_accuracy_batch = 0.0\n","\n","for batch_size in batch_sizes:\n","    model = CustomNet()\n","    optimizer = optim.SGD(model.parameters(), lr=0.001)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    for epoch in range(1, 11):\n","        train(epoch)\n","        validate()\n","\n","    # Check if the current batch size gives better validation accuracy\n","    if val_accuracies[-1] > best_val_accuracy_batch:\n","        best_batch_size = batch_size\n","        best_val_accuracy_batch = val_accuracies[-1]\n","\n","# Now, train the model with the best batch size using your original functions\n","model = CustomNet()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","for epoch in range(1, 11):\n","    train(epoch)\n","    validate()\n"]},{"cell_type":"markdown","metadata":{"id":"PJnJLKHNpXGT"},"source":["# Plotting for the best batch size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWcAFcGoorke"},"outputs":[],"source":["plt.figure(figsize=(12, 5))\n","plot_loss(train_losses, val_losses,f\"Losses for Batch Size: {best_batch_size}\")\n","plt.figure(figsize=(12, 5))\n","plot_accuracy(train_accuracies, val_accuracies,f\"Accuracies for Batch Size: {best_batch_size}\")\n","print(f\"Best Batch Size: {best_batch_size}, Final Validation Accuracy: {best_val_accuracy_batch:.2f}%\")\n"]},{"cell_type":"markdown","source":["#**Insights:**\n","\n","#1-Best Learning Rate (lr):\n","The model achieved the highest validation accuracy when trained with a learning rate of 0.01.\n","A learning rate that is too high can lead to overshooting, while a too-low learning rate may result in slow convergence.\n","The choice of the learning rate is crucial for achieving optimal model performance.\n","\n","#2-Best Batch Size (32):\n","The optimal batch size for training the model was found to be 32.\n","Batch size affects the stability of the training process and the memory requirements.\n","Larger batch sizes might lead to faster convergence but require more memory.\n","\n","#Final Validation Accuracy:\n","The model trained with the best hyperparameters achieved a final validation accuracy around [97]%.\n","The final accuracy is a key metric indicating the model's ability to generalize to unseen data."],"metadata":{"id":"5AJz2S2Kg3Rb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaqmpaoFQbaL"},"outputs":[],"source":["test_data = torch.from_numpy(testData).float().view(-1, 28 * 28)\n","test_labels = torch.from_numpy(testLabel)\n","# Create a TensorDataset for test data and labels\n","test_dataset = TensorDataset(test_data, test_labels)\n","\n","# Create a DataLoader for the test dataset\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","print(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9-iCwKsDY0f"},"outputs":[],"source":["def Test():\n","    model.eval()\n","    correct_test = 0\n","    total_test = 0\n","    test_loss = 0.0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            output = model(data)\n","            test_loss += criterion(output, target).item()\n","            _, predicted = output.max(1)\n","            total_test += target.size(0)\n","            correct_test += predicted.eq(target).sum().item()\n","\n","    test_accuracy = 100. * correct_test / total_test\n","    test_losses.append(test_loss / len(val_loader))\n","    test_accuracies.append(test_accuracy)\n","\n","    print('Test set: Average loss: {:.4f}, Accuracy: {:.2f}%'.format(val_losses[-1], test_accuracy))\n"]},{"cell_type":"code","source":["test_losses = []\n","test_accuracies = []\n","\n","for epoch in range(1, 11):\n","    train(epoch)\n","Test()"],"metadata":{"id":"foqN3YHQWrHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhEyQOOVtTjG"},"source":["#**Bonus**"]},{"cell_type":"markdown","metadata":{"id":"dqtgSdxw00vx"},"source":["* A modified architecture is introduced with dropout layers and layer normalization.\n","* Dropout helps prevent overfitting, and layer normalization normalizes inputs to hidden layers.\n","* This modified model is compared with the original in terms of performance."]},{"cell_type":"markdown","metadata":{"id":"y2O3VGzou25j"},"source":["# Modified Neural Network Architecture\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8Z45JHHtXZe"},"outputs":[],"source":["class CustomNetBonus(nn.Module):\n","    def __init__(self):\n","        super(CustomNetBonus, self).__init__()\n","        self.fc = nn.Linear(28 * 28, 256)\n","        self.fc1 = nn.Linear(256, 128)\n","        self.dropout1 = nn.Dropout(0.5)  # Add dropout with 50% probability\n","        self.layer_norm1 = nn.LayerNorm(128)  # Add layer normalization\n","        self.fc2 = nn.Linear(128, 64)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.layer_norm2 = nn.LayerNorm(64)\n","        self.fc3 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc(x))\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout1(x)\n","        x = self.layer_norm1(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout2(x)\n","        x = self.layer_norm2(x)\n","        x = F.relu(self.fc3(x))\n","        return F.log_softmax(x, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ir8Qp_9FvPfM"},"outputs":[],"source":["model_modified = CustomNetBonus()\n","optimizer_modified = optim.SGD(model_modified.parameters(), lr=0.01)\n","criterion_modified = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3CVsRFG5161"},"outputs":[],"source":["train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","for epoch in range(1, 11):\n","    train(epoch)\n","    validate()\n","    Test()\n","    print(f\"Epoch {epoch}: Train Loss {train_losses[-1]}, Val Loss {val_losses[-1]}, Val Acc {val_accuracies[-1]}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"yN_QkrQ-whf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYUh9abi5_mm"},"outputs":[],"source":["# Plotting for the modified architecture\n","plt.figure(figsize=(12, 5))\n","plot_loss(train_losses, val_losses,\"Modified Architecture Evaluation Loss\")\n","plt.figure(figsize=(12, 5))\n","plot_accuracy(train_accuracies, val_accuracies,\"Modified Architecture Evaluation Accuracy\")\n","print(\"Modified Architecture: Final Validation Accuracy {:.2f}%\".format(val_accuracies[-1]))"]}],"metadata":{"colab":{"provenance":[{"file_id":"1icjauoA9LNvUvNip3Eyf6goBlpP2cDEn","timestamp":1700318107654}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}